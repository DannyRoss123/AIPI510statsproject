# -*- coding: utf-8 -*-
"""AIPI510_Project2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AJ0hIEIT8t8djaO63JRqKKGbLN4hEMng

# Are All Random Number Generators Created Equal?

## Abstract
This notebook explores whether Python’s main random number generators (`random`, `numpy.random`, and `secrets`) actually behave the same statistically. We generated 1,000 random numbers from each and compared their distributions using chi-square and KS tests. The idea was simple: test if all of them produce equally uniform outputs, or if one deviates even slightly. The results show that all three are statistically uniform at this scale - though a power analysis suggests tiny differences could still exist below our detection limit.
"""

import random
import secrets

import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt

"""Now we create a simple config block. The three RNG algorithms we will be analyzing are: pythons, numpys, and secrets"""

# experiment config
N_PER_RNG = 1000
BINS = 10
RNGS = ["python", "numpy", "secrets"]

# for reproducibility (only affects python + numpy, not secrets)
random.seed(42)
np.random.seed(42)

"""Now its time to generate the random numbers. Each RNG creates 1,000 random values.  secrets.randbelow()` returns integers, so we divide by 10,000 to get a float in [0, 1).  All values go into one DataFrame with columns for RNG type and value.   """

rows = []

for rng in RNGS:
    if rng == "python":
        vals = [random.random() for _ in range(N_PER_RNG)]
    elif rng == "numpy":
        vals = np.random.random(size=N_PER_RNG).tolist()
    elif rng == "secrets":
        # secrets gives ints, so scale to [0,1)
        vals = [secrets.randbelow(10_000) / 10_000 for _ in range(N_PER_RNG)]
    else:
        raise ValueError(f"Unknown RNG: {rng}")

    for v in vals:
        rows.append({"rng": rng, "value": v})

df = pd.DataFrame(rows)
df.head()

"""
To apply chi-square tests, we need categorical bins.  
This divides the [0,1) range into 10 equal-width bins and assigns each value to a bin index (0–9). Uniform randomness means each bin should have about 100 samples.
"""

bin_edges = np.linspace(0, 1, BINS + 1)  # 0.0 ... 1.0
df["bin"] = pd.cut(df["value"], bins=bin_edges, labels=False, include_lowest=True)
df.head()

"""
Save raw and simple version of the datasets
"""

# raw
df.to_csv("rng_raw.csv", index=False)

# cleaned is basically the same here, but let's say we keep only needed cols
df_clean = df[["rng", "value", "bin"]].copy()
df_clean.to_csv("rng_clean.csv", index=False)

"""
For each RNG, we compared the observed bin counts to the expected uniform counts (100 per bin).  If randomness is perfect, the differences should be small.  We also computed Cohen’s w, which measures effect size (how far we are from uniform).  w < 0.1 → very small difference.
"""

results_gof = []

expected_count = N_PER_RNG / BINS  # 100

for rng in RNGS:
    obs = df[df["rng"] == rng]["bin"].value_counts().sort_index()
    # make sure we have 0..9
    obs = obs.reindex(range(BINS), fill_value=0)
    observed = obs.values

    # chi-square goodness-of-fit
    # scipy doesn't have a 1-liner for "against uniform expected counts", but we can do:
    chi2_stat, p_val = stats.chisquare(f_obs=observed,
                                       f_exp=np.full(BINS, expected_count))

    # effect size: w = sqrt(chi2 / n)
    w = np.sqrt(chi2_stat / N_PER_RNG)

    results_gof.append({
        "rng": rng,
        "chi2": chi2_stat,
        "p_value": p_val,
        "effect_size_w": w
    })

gof_df = pd.DataFrame(results_gof)
gof_df

"""All p-values are above 0.05, and the effect sizes are very small, showing no significant deviation from a uniform distribution. Even the Python RNG, with a slightly higher effect size (w ≈ 0.11), is still within normal random variation, meaning all three generators produce evenly distributed values across [0, 1).

"""

contingency = df.groupby(["rng", "bin"]).size().unstack(fill_value=0)
contingency

chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency)

chi2_ind_result = {
    "chi2": chi2_stat,
    "p_value": p_val,
    "dof": dof
}
chi2_ind_result

pairwise = []
for i in range(len(RNGS)):
    for j in range(i+1, len(RNGS)):
        rng1 = RNGS[i]
        rng2 = RNGS[j]
        v1 = df[df["rng"] == rng1]["value"].values
        v2 = df[df["rng"] == rng2]["value"].values

        ks_stat, p_val = stats.ks_2samp(v1, v2)

        pairwise.append({
            "rng1": rng1,
            "rng2": rng2,
            "ks_stat": ks_stat,
            "p_value": p_val
        })

ks_df = pd.DataFrame(pairwise)
ks_df

"""

The results (χ² = 22.99, p = 0.19, df = 18) again show **no significant difference** between the RNGs. A p-value above 0.05 means we fail to reject the null hypothesis — meaning RNG type and bin count are independent.

Put simply: all three RNGs generate numbers that are spread out in nearly identical ways. Any small variations in bin counts are well within what you’d expect from random chance. This supports the earlier finding that all three are equally uniform in practice.
"""

for rng in RNGS:
    subset = df[df["rng"] == rng]["value"]
    plt.figure()
    plt.hist(subset, bins=BINS, edgecolor="black")
    plt.title(f"Histogram of values – {rng}")
    plt.xlabel("Value")
    plt.ylabel("Count")
    plt.show()

contingency.T.plot(kind="bar", figsize=(8,5))
plt.xlabel("Bin")
plt.ylabel("Count")
plt.title("Counts per bin by RNG")
plt.show()

"""To check whether our sample size of 1,000 values per RNG is large enough to detect meaningful differences, we ran a statistical power analysis for the chi-square test. This helps estimate how likely our experiment is to catch a real deviation from uniformity if one exists.

"""

from statsmodels.stats.power import GofChisquarePower

# Initialize power calculator for chi-square goodness-of-fit
power_calc = GofChisquarePower()

# Compute statistical power for two effect sizes
power_small = power_calc.solve_power(effect_size=0.1, nobs=N_PER_RNG, n_bins=BINS, alpha=0.05)
power_tiny = power_calc.solve_power(effect_size=0.05, nobs=N_PER_RNG, n_bins=BINS, alpha=0.05)

print(f"Power to detect a small deviation (w=0.1): {power_small:.3f}")
print(f"Power to detect a very tiny deviation (w=0.05): {power_tiny:.3f}")

"""The test has about 56% power to detect small deviations (w = 0.1) and only 15% for very tiny ones (w = 0.05). This means our experiment could miss extremely subtle differences, but it’s still reasonably capable of identifying moderate deviations if they were present.

Overall, the results show that all three random number generators—Python’s built-in `random`, NumPy’s `random`, and `secrets`—produce statistically uniform and nearly identical distributions across the [0, 1) range. None of the tests found significant deviations or differences between them, and all effect sizes were very small. The power analysis suggests that while the experiment might miss extremely tiny differences, it’s strong enough to detect any meaningful bias. In short, all three RNGs perform equivalently for typical use cases.
"""